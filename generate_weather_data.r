# Writes a spark data frame containing yearly values of areal weather data.
# It consists of the following rows: 
# PRCP_year, TMAX_year, WTXX_year,
# PRCP_summer, TMAX_summer, WTXX_summer,
# PRCP_winter, TMAX_winter, WTXX_winter,
# Lat, Long and Year.
# Lat(itude) and Long(itude) are coordinates for the center of a tile (more on that below).
# PRCP is the amount of precipitation, TMAX is the maximum temperature and 
# WTXX is the mean of observed stormy weather types (defined in weather_data.r).
#
# The function processes the weather data taken from the
# Global Historical Climatology Network (GHCN) Daily data set 
# (one CSV file for each year).
# This data set contains daily measurements of weather stations.
# It's processed in the following way and then returned:
#
# The world map is partitioned into tiles of length tile_size (longitude, latitude)
# to generalize the weather measurements from a station based level to 
# a tile based level. The measured values of all stations that belong to
# the same tile are summarized using mean.
# From those tiled daily measurements the corresponding 
# baseline value is subtracted. The baseline is generated by 
# generate_baselines.r\generate_tiled_weather_baseline().
# It is basically a year of daily data generated from 
# multiple years using mean.
# The next step for the tiled daily data frame of baseline differences is the 
# generalization from a daily level to a yearly level. Therefore all daily 
# values within a year are summarized using mean. This results in a data frame
# where only one mean value exists for each unique combination of tile, year
# and measurment type.
# 
# Input:
# path_weather_yearly_org:  Source path to the yearly CSV Files of the GHCN Daily data set
# path_weather_data:  Target path to store the generated weather data
# path_stations_org:  Source path to the weather stations file taken 
#   from ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt
# tile_size:  Degrees of longitude/latitude defining the size of
#   each tile (quadrat) the world map is partitioned into
# path_tmp: Target path to store temporary data
# year_start_baseline:  Calendar year to mark the start of the baseline generation
# year_span_baseline: Number of years to define the span used for the baseline generation
# year_start_data:  Calendar year to mark the start of the weather data generation
#   used in the returned data frame
# year_end_data:  Calendar year to mark the end of the weather data generation
#   used in the returned data frame
# baseline_measurement_coverage_threshold:  Decimal number of range [0..1].
#   Defines during the baseline generation which tiles should be used,
#   because they have enough measurements to cover at least 
#   (baseline_measurement_coverage_threshold * 100) % of the baseline span.
generate_weather_data = function(
  path_weather_yearly_org,
  path_weather_data,
  path_stations_org,
  tile_size,
  path_tmp,
  year_start_baseline,
  year_span_baseline,
  year_start_data,
  year_end_data,
  baseline_measurement_coverage_threshold)
{
  # load functions
  source("init.r")
  source("weather_data.r")
  source("station.r")
  source("generate_baselines.r")
  source("generalization.r")
  source("initial_tiles.r")
  
  # Get paths, load packages and start spark connection
  init()
  path_tmp_baseline <- paste0( path_tmp, "baseline")
  
  sdf_tiled_stations <- 
    generate_tiled_stations_table( 
      path_source = path_stations_org, 
      tile_size = tile_size)
  
  # to skip the baseline generation and use an existing one,
  # comment the command below and uncomment the following one
  sdf_tiled_weather_baseline <-
    generate_tiled_weather_baseline(
      path_weather_files =
        path_weather_yearly_org,
      sdf_tiled_stations =
        sdf_tiled_stations,
      path_tmp_files =
        path_tmp,
      path_target =
        path_tmp_baseline,
      year_start_baseline =
        year_start_baseline,
      year_span_baseline =
        year_span_baseline,
      measurement_coverage_threshold =
        baseline_measurement_coverage_threshold)
  #sdf_tiled_weather_baseline <- read_tiled_weather_baseline( path_tmp_baseline)
  
  sdf_tiled_weather_baseline <- 
    rename(
      sdf_tiled_weather_baseline, 
      Value_baseline = Value)
  
  sdf_tiles_initial <- 
    sdf_tiled_weather_baseline %>% 
    group_by(Tile_Id) %>% 
    summarise()
  
  print( paste0( "Generating weather data (", 
                 year_start_data, "-", year_end_data, 
                 ") ..."))
  write_mode <- "overwrite"
  for(i in (year_start_data:year_end_data)) {
    sdf_weather_data <- 
      read_weather_data_org_with_tile_id( 
        path_weather_yearly_org, 
        i, 
        sdf_tiled_stations) %>%
      limit_data_to_considered_tiles( 
        sdf_considered_tiles = sdf_tiles_initial) %>% 
      generalize_from_stations_to_tiles_and_calc_baseline_differences(
        sdf_tiled_weather_baseline) %>%
      generalize_to_list_of_time_segments() %>%
      transform_for_output(
        sdf_tiled_stations,
        year = i
      )
    
    spark_write_csv(sdf_weather_data, path_weather_data, mode = write_mode)
    write_mode <- "append"
    print( paste0( "Wrote weather data for year ", i, "."))
  }
  print( "... Finished generating weather data.") 
}

# Collection of commands doing final transformations prior to writing the data.
transform_for_output = function (
  sdf_tiled_weather, 
  sdf_tiled_stations,
  year) 
{
  sdf_tiled_weather %>%
    transform_to_wide_table() %>%
    left_join(
      sdf_tiled_stations %>% 
        group_by(Tile_Id, Lat, Long) %>%
        summarize(), 
      by = c("Tile_Id")) %>% 
    select(-Tile_Id) %>%
    mutate(Year = year)
}

# Given a list of spark data frames, each one is tranformed by
# lspread_weather_elements() and all are joined to a single sdf.
transform_to_wide_table = function (
  list_sdf) 
{
  list_sdf <- 
    sapply(
      names(list_sdf), 
      lspread_weather_elements, 
      list_sdf_weather_data = list_sdf,
      simplify = FALSE,
      USE.NAMES = TRUE)
  
  sdf <- full_join_list(
    list_sdf,
    by = "Tile_Id"
  )
}

# Joins a list of data frames to a single data frame
full_join_list = function(
  list_sdf, 
  by)
{
  sdf <- list_sdf[[1]]
  if ( length( list_sdf) >= 2) {
    for(i in 2:length(list_sdf)) {
      sdf <- 
        full_join(
          sdf,
          list_sdf[[i]],
          by = by)
    }
  }
  sdf
}

# Given a list of weather data (spark data frames) and the name of one list element, 
# this single spark data frame is transformed and returned.
# The columns Element and Value are spread, so that each value of Element 
# becomes a column with the coressponding value of Value as its value (row).
# The former Element values, now column names are suffixed by "_" and name_list_element.
lspread_weather_elements = function(
  name_list_element, 
  list_sdf_weather_data)
{
  print(paste("Transforming list element:", name_list_element))
  list_sdf_weather_data[[name_list_element]] <- 
    list_sdf_weather_data[[name_list_element]] %>% 
    collect() %>% 
    spread( Element, Value) %>% 
    rename(
      !!paste0( "PRCP_", name_list_element) := PRCP, 
      !!paste0( "WTXX_", name_list_element) := WTXX, 
      !!paste0( "TMAX_", name_list_element) := TMAX) %>% 
    copy_to(dest = sc, name = paste0('weather_data_', name_list_element), overwrite = TRUE)
}